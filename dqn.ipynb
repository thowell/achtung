{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/taylor/Classes/cs230/achtung')\n",
    "import achtung;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "# from stable_baselines3.common.atari_wrappers import AtariWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achtung Die Kurve!\n"
     ]
    }
   ],
   "source": [
    "env = achtung.Achtung()\n",
    "env.speed = 0 # set to zero for training (i.e., no frame delay)\n",
    "env.render_game = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (3, 80, 80), uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 3)\n",
    "env = VecTransposeImage(env)\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"CnnPolicy\", \n",
    "            env, \n",
    "            buffer_size=10000,\n",
    "            learning_rate=1e-4,\n",
    "            batch_size=32,\n",
    "            learning_starts=100000,\n",
    "            target_update_interval=1000,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:33.44 +/- 11.97\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the initial random policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=25)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1\n",
      "   saving...\n",
      "   mean_reward:31.60 +/- 10.37\n",
      "iteration:  2\n",
      "   saving...\n",
      "   mean_reward:23.30 +/- 14.99\n",
      "iteration:  3\n",
      "   saving...\n",
      "   mean_reward:34.50 +/- 9.19\n",
      "iteration:  4\n",
      "   saving...\n",
      "   mean_reward:31.40 +/- 11.96\n",
      "iteration:  5\n",
      "   saving...\n",
      "   mean_reward:26.40 +/- 12.54\n",
      "iteration:  6\n",
      "   saving...\n",
      "   mean_reward:31.20 +/- 16.28\n",
      "iteration:  7\n",
      "   saving...\n",
      "   mean_reward:30.70 +/- 11.48\n",
      "iteration:  8\n",
      "   saving...\n",
      "   mean_reward:30.30 +/- 9.79\n",
      "iteration:  9\n",
      "   saving...\n",
      "   mean_reward:33.50 +/- 7.89\n",
      "iteration:  10\n",
      "   saving...\n",
      "   mean_reward:35.80 +/- 13.35\n",
      "iteration:  11\n",
      "   saving...\n",
      "   mean_reward:32.70 +/- 9.75\n",
      "iteration:  12\n",
      "   saving...\n",
      "   mean_reward:33.30 +/- 12.54\n",
      "iteration:  13\n",
      "   saving...\n",
      "   mean_reward:34.90 +/- 9.78\n",
      "iteration:  14\n",
      "   saving...\n",
      "   mean_reward:29.00 +/- 10.47\n",
      "iteration:  15\n",
      "   saving...\n",
      "   mean_reward:38.20 +/- 6.34\n",
      "iteration:  16\n",
      "   saving...\n",
      "   mean_reward:24.60 +/- 12.96\n",
      "iteration:  17\n",
      "   saving...\n",
      "   mean_reward:32.20 +/- 8.83\n",
      "iteration:  18\n",
      "   saving...\n",
      "   mean_reward:29.60 +/- 15.31\n",
      "iteration:  19\n",
      "   saving...\n",
      "   mean_reward:34.50 +/- 12.72\n",
      "iteration:  20\n",
      "   saving...\n",
      "   mean_reward:37.20 +/- 10.05\n",
      "iteration:  21\n",
      "   saving...\n",
      "   mean_reward:36.70 +/- 8.32\n",
      "iteration:  22\n",
      "   saving...\n",
      "   mean_reward:31.70 +/- 11.23\n",
      "iteration:  23\n",
      "   saving...\n",
      "   mean_reward:35.80 +/- 10.33\n",
      "iteration:  24\n",
      "   saving...\n",
      "   mean_reward:32.80 +/- 8.75\n",
      "iteration:  25\n",
      "   saving...\n",
      "   mean_reward:28.60 +/- 13.19\n",
      "iteration:  26\n",
      "   saving...\n",
      "   mean_reward:36.80 +/- 9.59\n",
      "iteration:  27\n",
      "   saving...\n",
      "   mean_reward:25.50 +/- 13.57\n",
      "iteration:  28\n",
      "   saving...\n",
      "   mean_reward:36.90 +/- 8.65\n",
      "iteration:  29\n",
      "   saving...\n",
      "   mean_reward:38.80 +/- 11.34\n",
      "iteration:  30\n",
      "   saving...\n",
      "   mean_reward:32.80 +/- 8.83\n",
      "iteration:  31\n",
      "   saving...\n",
      "   mean_reward:37.40 +/- 3.20\n",
      "iteration:  32\n",
      "   saving...\n",
      "   mean_reward:35.90 +/- 9.27\n",
      "iteration:  33\n",
      "   saving...\n",
      "   mean_reward:38.70 +/- 4.88\n",
      "iteration:  34\n",
      "   saving...\n",
      "   mean_reward:35.30 +/- 9.77\n",
      "iteration:  35\n",
      "   saving...\n",
      "   mean_reward:39.80 +/- 8.05\n",
      "iteration:  36\n",
      "   saving...\n",
      "   mean_reward:30.40 +/- 13.25\n",
      "iteration:  37\n",
      "   saving...\n",
      "   mean_reward:30.80 +/- 15.59\n",
      "iteration:  38\n",
      "   saving...\n",
      "   mean_reward:33.20 +/- 9.98\n",
      "iteration:  39\n",
      "   saving...\n",
      "   mean_reward:24.60 +/- 11.21\n",
      "iteration:  40\n",
      "   saving...\n",
      "   mean_reward:33.40 +/- 14.51\n",
      "iteration:  41\n",
      "   saving...\n",
      "   mean_reward:33.20 +/- 10.40\n",
      "iteration:  42\n",
      "   saving...\n",
      "   mean_reward:35.60 +/- 8.39\n",
      "iteration:  43\n",
      "   saving...\n",
      "   mean_reward:27.60 +/- 12.83\n",
      "iteration:  44\n",
      "   saving...\n",
      "   mean_reward:36.80 +/- 7.17\n",
      "iteration:  45\n",
      "   saving...\n",
      "   mean_reward:31.80 +/- 9.36\n",
      "iteration:  46\n",
      "   saving...\n",
      "   mean_reward:35.60 +/- 8.71\n",
      "iteration:  47\n",
      "   saving...\n",
      "   mean_reward:30.20 +/- 9.89\n",
      "iteration:  48\n",
      "   saving...\n",
      "   mean_reward:32.30 +/- 9.21\n",
      "iteration:  49\n",
      "   saving...\n",
      "   mean_reward:31.30 +/- 17.70\n",
      "iteration:  50\n",
      "   saving...\n",
      "   mean_reward:35.90 +/- 2.98\n",
      "iteration:  51\n",
      "   saving...\n",
      "   mean_reward:31.50 +/- 12.40\n",
      "iteration:  52\n",
      "   saving...\n",
      "   mean_reward:32.00 +/- 13.23\n",
      "iteration:  53\n",
      "   saving...\n",
      "   mean_reward:37.90 +/- 6.95\n",
      "iteration:  54\n",
      "   saving...\n",
      "   mean_reward:33.40 +/- 7.88\n",
      "iteration:  55\n",
      "   saving...\n",
      "   mean_reward:34.70 +/- 7.20\n",
      "iteration:  56\n",
      "   saving...\n",
      "   mean_reward:32.30 +/- 13.33\n",
      "iteration:  57\n",
      "   saving...\n",
      "   mean_reward:35.30 +/- 5.73\n",
      "iteration:  58\n",
      "   saving...\n",
      "   mean_reward:38.30 +/- 18.36\n",
      "iteration:  59\n",
      "   saving...\n",
      "   mean_reward:35.20 +/- 14.80\n",
      "iteration:  60\n",
      "   saving...\n",
      "   mean_reward:32.30 +/- 11.63\n",
      "iteration:  61\n",
      "   saving...\n",
      "   mean_reward:34.50 +/- 6.67\n",
      "iteration:  62\n",
      "   saving...\n",
      "   mean_reward:38.40 +/- 3.26\n",
      "iteration:  63\n",
      "   saving...\n",
      "   mean_reward:33.70 +/- 6.75\n",
      "iteration:  64\n",
      "   saving...\n",
      "   mean_reward:34.40 +/- 6.09\n",
      "iteration:  65\n",
      "   saving...\n",
      "   mean_reward:31.80 +/- 12.27\n",
      "iteration:  66\n",
      "   saving...\n",
      "   mean_reward:34.20 +/- 9.27\n",
      "iteration:  67\n",
      "   saving...\n",
      "   mean_reward:33.50 +/- 10.59\n",
      "iteration:  68\n",
      "   saving...\n",
      "   mean_reward:36.30 +/- 3.55\n",
      "iteration:  69\n",
      "   saving...\n",
      "   mean_reward:39.00 +/- 4.15\n",
      "iteration:  70\n",
      "   saving...\n",
      "   mean_reward:33.60 +/- 10.51\n",
      "iteration:  71\n",
      "   saving...\n",
      "   mean_reward:32.60 +/- 10.11\n",
      "iteration:  72\n",
      "   saving...\n",
      "   mean_reward:37.40 +/- 9.29\n",
      "iteration:  73\n",
      "   saving...\n",
      "   mean_reward:37.90 +/- 4.13\n",
      "iteration:  74\n",
      "   saving...\n",
      "   mean_reward:35.80 +/- 10.77\n",
      "iteration:  75\n",
      "   saving...\n",
      "   mean_reward:36.70 +/- 11.60\n",
      "iteration:  76\n",
      "   saving...\n",
      "   mean_reward:31.60 +/- 10.65\n",
      "iteration:  77\n",
      "   saving...\n",
      "   mean_reward:38.70 +/- 8.00\n",
      "iteration:  78\n",
      "   saving...\n",
      "   mean_reward:33.30 +/- 9.48\n",
      "iteration:  79\n",
      "   saving...\n",
      "   mean_reward:34.30 +/- 9.78\n",
      "iteration:  80\n",
      "   saving...\n",
      "   mean_reward:32.00 +/- 8.51\n",
      "iteration:  81\n",
      "   saving...\n",
      "   mean_reward:31.10 +/- 13.10\n",
      "iteration:  82\n",
      "   saving...\n",
      "   mean_reward:35.40 +/- 10.46\n",
      "iteration:  83\n",
      "   saving...\n",
      "   mean_reward:36.60 +/- 10.31\n",
      "iteration:  84\n",
      "   saving...\n",
      "   mean_reward:35.20 +/- 7.25\n",
      "iteration:  85\n",
      "   saving...\n",
      "   mean_reward:39.00 +/- 6.10\n",
      "iteration:  86\n",
      "   saving...\n",
      "   mean_reward:33.10 +/- 14.62\n",
      "iteration:  87\n",
      "   saving...\n",
      "   mean_reward:31.60 +/- 12.96\n",
      "iteration:  88\n",
      "   saving...\n",
      "   mean_reward:33.50 +/- 12.31\n",
      "iteration:  89\n",
      "   saving...\n",
      "   mean_reward:34.30 +/- 10.43\n",
      "iteration:  90\n",
      "   saving...\n",
      "   mean_reward:33.80 +/- 5.10\n",
      "iteration:  91\n",
      "   saving...\n",
      "   mean_reward:38.50 +/- 3.56\n",
      "iteration:  92\n",
      "   saving...\n",
      "   mean_reward:32.00 +/- 15.51\n",
      "iteration:  93\n",
      "   saving...\n",
      "   mean_reward:32.40 +/- 11.62\n",
      "iteration:  94\n",
      "   saving...\n",
      "   mean_reward:40.80 +/- 6.65\n",
      "iteration:  95\n",
      "   saving...\n",
      "   mean_reward:32.90 +/- 14.01\n",
      "iteration:  96\n",
      "   saving...\n",
      "   mean_reward:31.20 +/- 8.51\n",
      "iteration:  97\n",
      "   saving...\n",
      "   mean_reward:40.30 +/- 6.83\n",
      "iteration:  98\n",
      "   saving...\n",
      "   mean_reward:29.80 +/- 14.01\n",
      "iteration:  99\n",
      "   saving...\n",
      "   mean_reward:32.50 +/- 9.27\n",
      "iteration:  100\n",
      "   saving...\n",
      "   mean_reward:32.90 +/- 12.20\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "rewards = []\n",
    "stds = []\n",
    "for i in range(100):\n",
    "    print(\"iteration: \", i+1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    print(\"   saving...\")\n",
    "    model.save(\"dqn_achtung\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
    "    print(f\"   mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    rewards.append(mean_reward)\n",
    "    stds.append(std_reward)\n",
    "    \n",
    "     \n",
    "    with open(\"baselines_training/dqn_reward.txt\", \"wb\") as f:   \n",
    "        pickle.dump(rewards, f)\n",
    "    with open(\"baselines_training/dqn_std.txt\", \"wb\") as f:   \n",
    "        pickle.dump(stds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:33.80 +/- 8.21\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
