{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/taylor/Classes/cs230/achtung')\n",
    "import achtung;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "# from stable_baselines3.common.atari_wrappers import AtariWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achtung Die Kurve!\n"
     ]
    }
   ],
   "source": [
    "env = achtung.Achtung()\n",
    "env.speed = 0 # set to zero for training (i.e., no frame delay)\n",
    "env.render_game = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (3, 80, 80), uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 3)\n",
    "env = VecTransposeImage(env)\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\"CnnPolicy\", \n",
    "            env, \n",
    "            ent_coef=0.01,\n",
    "            vf_coef=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:52.84 +/- 27.21\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the initial random policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=25)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1\n",
      "   saving...\n",
      "   mean_reward:35.00 +/- 12.39\n",
      "iteration:  2\n",
      "   saving...\n",
      "   mean_reward:42.70 +/- 6.44\n",
      "iteration:  3\n",
      "   saving...\n",
      "   mean_reward:28.20 +/- 13.70\n",
      "iteration:  4\n",
      "   saving...\n",
      "   mean_reward:37.10 +/- 4.53\n",
      "iteration:  5\n",
      "   saving...\n",
      "   mean_reward:33.50 +/- 10.80\n",
      "iteration:  6\n",
      "   saving...\n",
      "   mean_reward:49.40 +/- 23.38\n",
      "iteration:  7\n",
      "   saving...\n",
      "   mean_reward:38.10 +/- 4.81\n",
      "iteration:  8\n",
      "   saving...\n",
      "   mean_reward:54.80 +/- 19.61\n",
      "iteration:  9\n",
      "   saving...\n",
      "   mean_reward:40.60 +/- 23.15\n",
      "iteration:  10\n",
      "   saving...\n",
      "   mean_reward:54.10 +/- 31.96\n",
      "iteration:  11\n",
      "   saving...\n",
      "   mean_reward:53.80 +/- 29.13\n",
      "iteration:  12\n",
      "   saving...\n",
      "   mean_reward:34.00 +/- 13.73\n",
      "iteration:  13\n",
      "   saving...\n",
      "   mean_reward:38.90 +/- 6.11\n",
      "iteration:  14\n",
      "   saving...\n",
      "   mean_reward:30.40 +/- 11.59\n",
      "iteration:  15\n",
      "   saving...\n",
      "   mean_reward:36.70 +/- 4.56\n",
      "iteration:  16\n",
      "   saving...\n",
      "   mean_reward:52.50 +/- 30.72\n",
      "iteration:  17\n",
      "   saving...\n",
      "   mean_reward:53.10 +/- 24.05\n",
      "iteration:  18\n",
      "   saving...\n",
      "   mean_reward:33.60 +/- 6.48\n",
      "iteration:  19\n",
      "   saving...\n",
      "   mean_reward:30.50 +/- 13.14\n",
      "iteration:  20\n",
      "   saving...\n",
      "   mean_reward:30.20 +/- 9.58\n",
      "iteration:  21\n",
      "   saving...\n",
      "   mean_reward:31.90 +/- 14.22\n",
      "iteration:  22\n",
      "   saving...\n",
      "   mean_reward:33.30 +/- 9.66\n",
      "iteration:  23\n",
      "   saving...\n",
      "   mean_reward:32.00 +/- 11.67\n",
      "iteration:  24\n",
      "   saving...\n",
      "   mean_reward:29.10 +/- 12.87\n",
      "iteration:  25\n",
      "   saving...\n",
      "   mean_reward:38.80 +/- 6.19\n",
      "iteration:  26\n",
      "   saving...\n",
      "   mean_reward:23.30 +/- 13.91\n",
      "iteration:  27\n",
      "   saving...\n",
      "   mean_reward:38.00 +/- 8.11\n",
      "iteration:  28\n",
      "   saving...\n",
      "   mean_reward:27.20 +/- 13.04\n",
      "iteration:  29\n",
      "   saving...\n",
      "   mean_reward:62.70 +/- 29.54\n",
      "iteration:  30\n",
      "   saving...\n",
      "   mean_reward:30.20 +/- 13.15\n",
      "iteration:  31\n",
      "   saving...\n",
      "   mean_reward:58.30 +/- 27.39\n",
      "iteration:  32\n",
      "   saving...\n",
      "   mean_reward:45.80 +/- 28.79\n",
      "iteration:  33\n",
      "   saving...\n",
      "   mean_reward:42.30 +/- 27.27\n",
      "iteration:  34\n",
      "   saving...\n",
      "   mean_reward:36.70 +/- 10.86\n",
      "iteration:  35\n",
      "   saving...\n",
      "   mean_reward:38.40 +/- 6.41\n",
      "iteration:  36\n",
      "   saving...\n",
      "   mean_reward:32.40 +/- 22.72\n",
      "iteration:  37\n",
      "   saving...\n",
      "   mean_reward:36.80 +/- 15.94\n",
      "iteration:  38\n",
      "   saving...\n",
      "   mean_reward:34.40 +/- 9.85\n",
      "iteration:  39\n",
      "   saving...\n",
      "   mean_reward:33.60 +/- 13.20\n",
      "iteration:  40\n",
      "   saving...\n",
      "   mean_reward:36.50 +/- 3.29\n",
      "iteration:  41\n",
      "   saving...\n",
      "   mean_reward:35.60 +/- 10.02\n",
      "iteration:  42\n",
      "   saving...\n",
      "   mean_reward:33.60 +/- 8.27\n",
      "iteration:  43\n",
      "   saving...\n",
      "   mean_reward:30.60 +/- 22.54\n",
      "iteration:  44\n",
      "   saving...\n",
      "   mean_reward:26.20 +/- 15.08\n",
      "iteration:  45\n",
      "   saving...\n",
      "   mean_reward:22.80 +/- 15.99\n",
      "iteration:  46\n",
      "   saving...\n",
      "   mean_reward:31.90 +/- 8.89\n",
      "iteration:  47\n",
      "   saving...\n",
      "   mean_reward:48.40 +/- 25.02\n",
      "iteration:  48\n",
      "   saving...\n",
      "   mean_reward:36.80 +/- 10.65\n",
      "iteration:  49\n",
      "   saving...\n",
      "   mean_reward:38.50 +/- 5.77\n",
      "iteration:  50\n",
      "   saving...\n",
      "   mean_reward:30.90 +/- 13.32\n",
      "iteration:  51\n",
      "   saving...\n",
      "   mean_reward:51.10 +/- 21.64\n",
      "iteration:  52\n",
      "   saving...\n",
      "   mean_reward:36.00 +/- 8.29\n",
      "iteration:  53\n",
      "   saving...\n",
      "   mean_reward:31.20 +/- 13.62\n",
      "iteration:  54\n",
      "   saving...\n",
      "   mean_reward:36.80 +/- 8.32\n",
      "iteration:  55\n",
      "   saving...\n",
      "   mean_reward:41.90 +/- 24.41\n",
      "iteration:  56\n",
      "   saving...\n",
      "   mean_reward:24.40 +/- 15.25\n",
      "iteration:  57\n",
      "   saving...\n",
      "   mean_reward:40.80 +/- 25.88\n",
      "iteration:  58\n",
      "   saving...\n",
      "   mean_reward:36.30 +/- 11.52\n",
      "iteration:  59\n",
      "   saving...\n",
      "   mean_reward:35.00 +/- 9.74\n",
      "iteration:  60\n",
      "   saving...\n",
      "   mean_reward:63.70 +/- 21.35\n",
      "iteration:  61\n",
      "   saving...\n",
      "   mean_reward:37.40 +/- 4.82\n",
      "iteration:  62\n",
      "   saving...\n",
      "   mean_reward:42.00 +/- 24.97\n",
      "iteration:  63\n",
      "   saving...\n",
      "   mean_reward:50.30 +/- 27.20\n",
      "iteration:  64\n",
      "   saving...\n",
      "   mean_reward:39.90 +/- 5.87\n",
      "iteration:  65\n",
      "   saving...\n",
      "   mean_reward:35.80 +/- 2.48\n",
      "iteration:  66\n",
      "   saving...\n",
      "   mean_reward:40.20 +/- 4.31\n",
      "iteration:  67\n",
      "   saving...\n",
      "   mean_reward:32.60 +/- 12.63\n",
      "iteration:  68\n",
      "   saving...\n",
      "   mean_reward:31.00 +/- 13.93\n",
      "iteration:  69\n",
      "   saving...\n",
      "   mean_reward:36.70 +/- 2.57\n",
      "iteration:  70\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "rewards = []\n",
    "stds = []\n",
    "for i in range(100):\n",
    "    print(\"iteration: \", i+1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    print(\"   saving...\")\n",
    "    model.save(\"a2c_achtung\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
    "    print(f\"   mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    rewards.append(mean_reward)\n",
    "    stds.append(std_reward)\n",
    "    \n",
    "    with open(\"baselines_training/a2c_reward.txt\", \"wb\") as f:   \n",
    "        pickle.dump(rewards, f)\n",
    "    with open(\"baselines_training/a2c_std.txt\", \"wb\") as f:   \n",
    "        pickle.dump(stds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
