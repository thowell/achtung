{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/taylor/Classes/cs230/achtung')\n",
    "import achtung;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "# from stable_baselines3.common.atari_wrappers import AtariWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achtung Die Kurve!\n"
     ]
    }
   ],
   "source": [
    "env = achtung.Achtung()\n",
    "env.speed = 0 # set to zero for training (i.e., no frame delay)\n",
    "env.render_game = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50):\n",
    "#     action = 0\n",
    "#     obs, rewards, done, info = env.step(action)\n",
    "#     env.render()\n",
    "    \n",
    "#     plt.imshow(np.resize(obs, (achtung.OBS_HEIGHT, achtung.OBS_WIDTH)), cmap=\"gray\") \n",
    "#     plt.show()\n",
    "\n",
    "#     if done:\n",
    "#         obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (3, 80, 80), uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 3)\n",
    "env = VecTransposeImage(env)\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"CnnPolicy\", \n",
    "            env, \n",
    "            n_steps=128,\n",
    "            n_epochs=4,\n",
    "            batch_size=256,\n",
    "            learning_rate=2.5e-4,\n",
    "            clip_range=0.1,\n",
    "            clip_range_vf=0.1,\n",
    "            vf_coef=0.5,\n",
    "            ent_coef=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:60.64 +/- 31.81\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the initial random policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=25)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:29.50 +/- 12.33\n",
      "iteration:  2\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:35.10 +/- 7.92\n",
      "iteration:  3\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:40.90 +/- 5.84\n",
      "iteration:  4\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:34.50 +/- 11.32\n",
      "iteration:  5\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:54.90 +/- 23.90\n",
      "iteration:  6\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:26.00 +/- 14.16\n",
      "iteration:  7\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:25.40 +/- 25.44\n",
      "iteration:  8\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:34.50 +/- 10.32\n",
      "iteration:  9\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:34.30 +/- 11.89\n",
      "iteration:  10\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:33.00 +/- 13.42\n",
      "iteration:  11\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:40.20 +/- 23.01\n",
      "iteration:  12\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:31.70 +/- 13.31\n",
      "iteration:  13\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:57.40 +/- 22.63\n",
      "iteration:  14\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:33.30 +/- 10.09\n",
      "iteration:  15\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.70 +/- 5.87\n",
      "iteration:  16\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:32.50 +/- 12.27\n",
      "iteration:  17\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:50.30 +/- 31.10\n",
      "iteration:  18\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:47.10 +/- 30.38\n",
      "iteration:  19\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:48.40 +/- 30.18\n",
      "iteration:  20\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.60 +/- 22.93\n",
      "iteration:  21\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:48.80 +/- 28.29\n",
      "iteration:  22\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:49.30 +/- 17.74\n",
      "iteration:  23\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:30.70 +/- 25.29\n",
      "iteration:  24\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:42.70 +/- 24.85\n",
      "iteration:  25\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:44.70 +/- 19.14\n",
      "iteration:  26\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:43.70 +/- 18.06\n",
      "iteration:  27\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.90 +/- 20.16\n",
      "iteration:  28\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.90 +/- 19.31\n",
      "iteration:  29\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:45.00 +/- 18.43\n",
      "iteration:  30\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:40.60 +/- 16.64\n",
      "iteration:  31\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:31.30 +/- 14.81\n",
      "iteration:  32\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.20 +/- 14.54\n",
      "iteration:  33\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:46.30 +/- 19.40\n",
      "iteration:  34\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:49.10 +/- 22.05\n",
      "iteration:  35\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:42.70 +/- 18.46\n",
      "iteration:  36\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:35.40 +/- 17.39\n",
      "iteration:  37\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:58.50 +/- 29.74\n",
      "iteration:  38\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:44.70 +/- 25.04\n",
      "iteration:  39\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.80 +/- 12.88\n",
      "iteration:  40\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:34.10 +/- 13.52\n",
      "iteration:  41\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:34.00 +/- 13.84\n",
      "iteration:  42\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.50 +/- 19.09\n",
      "iteration:  43\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:35.20 +/- 21.40\n",
      "iteration:  44\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:46.90 +/- 25.12\n",
      "iteration:  45\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:50.60 +/- 24.12\n",
      "iteration:  46\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:57.10 +/- 22.64\n",
      "iteration:  47\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:62.40 +/- 22.50\n",
      "iteration:  48\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:52.20 +/- 22.71\n",
      "iteration:  49\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:46.50 +/- 23.14\n",
      "iteration:  50\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.40 +/- 25.70\n",
      "iteration:  51\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:39.90 +/- 25.30\n",
      "iteration:  52\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.90 +/- 23.39\n",
      "iteration:  53\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:54.60 +/- 22.71\n",
      "iteration:  54\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:54.00 +/- 22.53\n",
      "iteration:  55\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:46.10 +/- 16.96\n",
      "iteration:  56\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:50.00 +/- 14.61\n",
      "iteration:  57\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:62.20 +/- 25.51\n",
      "iteration:  58\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:49.50 +/- 22.61\n",
      "iteration:  59\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.40 +/- 20.82\n",
      "iteration:  60\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:45.50 +/- 22.80\n",
      "iteration:  61\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:45.80 +/- 12.03\n",
      "iteration:  62\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:39.50 +/- 27.77\n",
      "iteration:  63\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:51.70 +/- 21.30\n",
      "iteration:  64\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:35.50 +/- 10.30\n",
      "iteration:  65\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:36.00 +/- 6.88\n",
      "iteration:  66\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.40 +/- 11.69\n",
      "iteration:  67\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.90 +/- 13.13\n",
      "iteration:  68\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:28.30 +/- 16.23\n",
      "iteration:  69\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:30.00 +/- 14.06\n",
      "iteration:  70\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:39.00 +/- 6.84\n",
      "iteration:  71\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:36.90 +/- 12.96\n",
      "iteration:  72\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:39.10 +/- 12.20\n",
      "iteration:  73\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:36.10 +/- 15.97\n",
      "iteration:  74\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:45.10 +/- 7.58\n",
      "iteration:  75\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:42.60 +/- 13.53\n",
      "iteration:  76\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:39.40 +/- 18.88\n",
      "iteration:  77\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:40.30 +/- 14.86\n",
      "iteration:  78\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:48.80 +/- 16.67\n",
      "iteration:  79\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:39.60 +/- 15.83\n",
      "iteration:  80\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.00 +/- 15.20\n",
      "iteration:  81\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:36.80 +/- 16.33\n",
      "iteration:  82\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:45.30 +/- 9.68\n",
      "iteration:  83\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:48.90 +/- 8.23\n",
      "iteration:  84\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:54.30 +/- 6.60\n",
      "iteration:  85\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.80 +/- 22.28\n",
      "iteration:  86\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:45.70 +/- 19.52\n",
      "iteration:  87\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:62.30 +/- 31.91\n",
      "iteration:  88\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:40.70 +/- 24.29\n",
      "iteration:  89\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:48.30 +/- 23.04\n",
      "iteration:  90\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:30.50 +/- 17.96\n",
      "iteration:  91\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:32.00 +/- 19.59\n",
      "iteration:  92\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:32.60 +/- 13.21\n",
      "iteration:  93\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:38.40 +/- 3.07\n",
      "iteration:  94\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:30.20 +/- 11.98\n",
      "iteration:  95\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:37.40 +/- 6.39\n",
      "iteration:  96\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:49.40 +/- 6.14\n",
      "iteration:  97\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:40.30 +/- 23.55\n",
      "iteration:  98\n",
      "   saving...\n",
      "   evalute...\n",
      "   mean_reward:57.20 +/- 21.47\n",
      "iteration:  99\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "rewards = []\n",
    "stds = []\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"iteration: \", i+1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    print(\"   saving...\")\n",
    "    model.save(\"ppo_achtung\")\n",
    "    \n",
    "    print(\"   evalute...\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic = True)\n",
    "    print(f\"   mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    rewards.append(mean_reward)\n",
    "    stds.append(std_reward)\n",
    "    \n",
    "    with open(\"baselines_training/ppo_reward.txt\", \"wb\") as f:   \n",
    "        pickle.dump(rewards, f)\n",
    "    with open(\"baselines_training/ppo_std.txt\", \"wb\") as f:   \n",
    "        pickle.dump(stds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained policy\n",
    "model.load(\"ppo_achtung\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic = True)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
